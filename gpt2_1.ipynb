{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHaphzf+HdqSNX6PdUyMCt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/winny0217/gpt2/blob/main/gpt2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SkOYjOCLT4c8"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# 載入模型與 tokenizer\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "model.eval()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model.to(\"cuda\")\n",
        "\n",
        "def get_top_k_predictions(input_text, top_k=3, max_predict_len=3):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "    if torch.cuda.is_available():\n",
        "        input_ids = input_ids.to(\"cuda\")\n",
        "\n",
        "    generated_ids = input_ids.clone()\n",
        "\n",
        "    suggestions = []\n",
        "\n",
        "    for _ in range(max_predict_len):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(generated_ids)\n",
        "            logits = outputs.logits\n",
        "\n",
        "        next_token_logits = logits[:, -1, :]  # 最後一個 token 的預測\n",
        "        probs = torch.softmax(next_token_logits, dim=-1)\n",
        "\n",
        "        # 選出 top_k 的 token\n",
        "        top_k_probs, top_k_ids = torch.topk(probs, top_k)\n",
        "\n",
        "        # 解碼成字詞\n",
        "        tokens = [tokenizer.decode([token_id]) for token_id in top_k_ids[0]]\n",
        "        suggestions.append(tokens)\n",
        "\n",
        "        # 模擬「選擇第一個建議」\n",
        "        selected_id = top_k_ids[0][0].unsqueeze(0).unsqueeze(0)\n",
        "        generated_ids = torch.cat((generated_ids, selected_id), dim=1)\n",
        "\n",
        "    return suggestions"
      ],
      "metadata": {
        "id": "nnHGxESWUBXT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== 示範輸入 ====\n",
        "input_text = \"I want to go to the\"\n",
        "\n",
        "suggestions = get_top_k_predictions(input_text)\n",
        "\n",
        "print(f\"Input text: {input_text}\")\n",
        "for i, options in enumerate(suggestions):\n",
        "    print(f\"Suggestion for word {i+1}: {options}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRLVNEPvUJH_",
        "outputId": "8cc09ad2-33a2-4042-cab1-d1f75b60fa04"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input text: I want to go to the\n",
            "Suggestion for word 1: [' next', ' gym', ' hospital']\n",
            "Suggestion for word 2: [' level', ' stage', ' step']\n",
            "Suggestion for word 3: ['.', ',\"', ',']\n"
          ]
        }
      ]
    }
  ]
}